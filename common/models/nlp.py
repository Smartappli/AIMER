model_id=76
match model_id:
    case 76:
        model_name = "Alfred-40B"
        repo = "TheBloke/alfred-40B-1023-GGUF"
        quantization = "q4_K_M"
        model_type = "GGUF"
        filename = ["alfred-40b-1023.Q4_K_M.gguf"]
        filesize = [25452369600.0]

    case 77:
        model_name = "Code-13B"
        repo = "TheBloke/Code-13B-GGUF"
        quantization = "q4_K_M"
        model_type = "GGUF"
        filename = ["code-13b.Q4_K_M.gguf"]
        filesize = [7865956352.0]

    case 78:
        model_name = "Code-33B"
        repo = "TheBloke/Code-33B-GGUF"
        quantization = "q4_K_M"
        model_type = "GGUF"
        filename = ["code-33b.Q4_K_M.gguf"]
        filesize = [19621140000.0]

    case 79:
        model_name = "CodeLLaMA-7B"
        repo = "TheBloke/CodeLlama-7B-GGUF"
        quantization = "q4_K_M"
        model_type = "GGUF"
        filename = ["codellama-7b.Q4_K_M.gguf"]
        filesize = [4081095360.0]

    case 80:
        model_name = "CodeLLaMA-7B-Instruct"
        repo = "TheBloke/CodeLlama-7B-Instruct-GGUF"
        quantization = "q4_K_M"
        model_type = "GGUF"
        filename = ["codellama-7b-instruct.Q4_K_M.gguf"]
        filesize = [4081095360.0]

    case 81:
        model_name = "CodeLLaMA-13B"
        repo = "TheBloke/CodeLlama-13B-GGUF"
        quantization = "q4_K_M"
        model_type = "GGUF"
        filename = ["codellama-13b.Q4_K_M.gguf"]
        filesize = [7866070016.0]

    case 82:
        model_name = "CodeLLaMA-13B-Instruct"
        repo = "TheBloke/CodeLlama-13B-Instruct-GGUF"
        quantization = "q4_K_M"
        model_type = "GGUF"
        filename = ["codellama-13b-instruct.Q4_K_M.gguf"]
        filesize = [7866070016.0]

    case 83:
        model_name = "CodeLLaMA-34B"
        repo = "TheBloke/CodeLlama-34B-GGUF"
        quantization = "q4_K_M"
        model_type = "GGUF"
        filename = ["codellama-34b.Q4_K_M.gguf"]
        filesize = [20219900064.0]

    case 84:
        model_name = "CodeLLaMA-34B-Instruct"
        repo = "codellama-34b-instruct.Q4_K_M.gguf"
        quantization = "q4_K_M"
        model_type = "GGUF"
        filename = ["codellama-34b-instruct.Q4_K_M.gguf"]
        filesize = [20219900064.0]